{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DLOI_CW6_HirokiHayakawa.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"p45UjNjFyoHc","colab_type":"text"},"source":["# Coursework 6"]},{"cell_type":"markdown","metadata":{"id":"7CNJoJ0NBfPj","colab_type":"text"},"source":["## Task 1"]},{"cell_type":"markdown","metadata":{"id":"0mIcpL6pLPvL","colab_type":"text"},"source":["### Import required Python libraries\n","\n"]},{"cell_type":"code","metadata":{"id":"jaV4mHBXVj2x","colab_type":"code","colab":{}},"source":["import numpy as np\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, Activation\n","from tensorflow.python.keras.layers import Conv2D, MaxPooling2D\n","from tensorflow.keras.layers import Dense, Dropout, Flatten\n","from tensorflow.keras.optimizers import SGD"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BVMwTVUOV1KY","colab_type":"text"},"source":["### Import and shape the dataset (Fashion Mnist)"]},{"cell_type":"code","metadata":{"id":"lxMLelekV8xh","colab_type":"code","colab":{}},"source":["print('* Importing and shaping the data *')\n","print()\n","\n","mnist = tf.keras.datasets.fashion_mnist  # load mnist dataset from tensorflow\n","(input_train, output_train_class), (input_test, output_test_class) = mnist.load_data()\n","\n","print('input_train (original): ', input_train.shape)\n","print('input_test (original): ', input_test.shape)\n","print()\n","\n","input_train= input_train.reshape(input_train.shape[0], 28, 28, 1)  # add an extra dimension to array\n","input_test= input_test.reshape(input_test.shape[0], 28, 28, 1)\n","\n","input_train = input_train / 255.0  # max normalise the image data[0:1]\n","input_test = input_test / 255.0\n","\n","output_train_class_onehot = tf.keras.utils.to_categorical(output_train_class, 10)  # create one-hot encoded class\n","output_test_class_onehot = tf.keras.utils.to_categorical(output_test_class, 10)\n","\n","output_class_names = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']    # class names string\n","\n","print('input_train : ', input_train.shape)\n","print('output_train_class : ', output_train_class.shape)\n","print('output_train_class_onehot : ', output_train_class_onehot.shape)\n","print()\n","print('input_test : ', input_test.shape)\n","print('output_test_class : ', output_test_class.shape)\n","print('output_test_class_onehot : ', output_test_class_onehot.shape)\n","print()\n","print('output_class_names : ', output_class_names)\n","print()\n","\n","item_id = 5\n","\n","print('item_id : ', item_id)\n","print('output_train_class [item_id] : ', output_train_class[item_id])\n","print('output_train_class_onehot [item_id] : ', output_train_class_onehot[item_id, :])\n","\n","plt.imshow(input_train[item_id, :, :, 0], cmap=plt.cm.binary)\n","plt.title('input_train [' + str(item_id) + ']')\n","plt.grid(None)\n","plt.xticks([])\n","plt.yticks([])\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kgB--wW0dGPU","colab_type":"text"},"source":["### Define the network hyperparameters\n"]},{"cell_type":"code","metadata":{"id":"GdlhlPRPdZck","colab_type":"code","colab":{}},"source":["optimizer_type = SGD(lr=0.2)  # optimisation algorithm: SGD stochastic gradient decent \n","loss = 'categorical_crossentropy'  # loss (cost) function to be minimised by the optimiser\n","metrics = ['categorical_accuracy']  # network accuracy metric to be determined after each epoch\n","dropout_ratio = 0.0  # % of nodes in the hidden layer to dropout during back-propagation update of the network weights\n","validtrain_split_ratio = 0.2  # % of the seen dataset to be put aside for validation, rest is for training\n","max_epochs = 50  # maxmimum number of epochs to be iterated\n","batch_size = 500   # batch size for the training data set\n","batch_shuffle = True   # shuffle the training data prior to batching before each epoch\n","num_hidden_nodes = 256  # number of nodes in hidden fully connected layer"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y6JHphuLcvb6","colab_type":"text"},"source":["### Define the network architecture\n"]},{"cell_type":"code","metadata":{"id":"TVOQwMTkdrdb","colab_type":"code","colab":{}},"source":["input_shape = (28, 28, 1)\n","inputs = Input(shape=input_shape)\n","\n","down_01 = Conv2D(filters=16, kernel_size=(3, 3), strides=(1, 1), padding='same')(inputs)\n","down_01 = Activation('relu')(down_01)\n","down_01 = Conv2D(filters=16, kernel_size=(3, 3), strides=(1, 1), padding='same')(down_01)\n","down_01 = Activation('relu')(down_01)\n","\n","down_01_pool = MaxPooling2D((2, 2), strides=(2, 2))(down_01)   # maxpool downsampled to 14x14x16\n","\n","down_02 = Conv2D(filters=32, kernel_size=(3, 3), strides=(1, 1), padding='same')(down_01_pool)\n","down_02 = Activation('relu')(down_02)\n","down_02 = Conv2D(filters=32, kernel_size=(3, 3), strides=(1, 1), padding='same')(down_02)\n","down_02 = Activation('relu')(down_02)\n","\n","down_02_pool = MaxPooling2D((2, 2), strides=(2, 2))(down_02)   # maxpool downsampled to 7x7x32\n","\n","flatten = Flatten()(down_02_pool)   # 1568 nodes\n","\n","dense_01 = Dense(num_hidden_nodes)(flatten)\n","dense_01 = Activation('sigmoid')(dense_01)\n","dense_01 = Dropout(dropout_ratio)(dense_01)\n","\n","dense_02 = Dense(10)(dense_01)\n","outputs = Activation('softmax')(dense_02)\n","\n","# compile the network\n","model = Model(inputs=inputs, outputs=outputs)\n","model.compile(optimizer=optimizer_type, loss=loss, metrics=metrics)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TvfyQk9LgaEz","colab_type":"text"},"source":["### Train the neural network with the training dataset"]},{"cell_type":"code","metadata":{"id":"qmXxAsVZggoD","colab_type":"code","colab":{}},"source":["print('* Training the compiled network *')\n","print()\n","\n","history = model.fit(input_train, output_train_class_onehot, \\\n","                    batch_size=batch_size, \\\n","                    epochs=max_epochs, \\\n","                    validation_split=validtrain_split_ratio, \\\n","                    shuffle=batch_shuffle)\n","\n","print()\n","print('Training completed')\n","print()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fT1FYewGwOo6","colab_type":"text"},"source":["### Plot the training history of the network\n"]},{"cell_type":"code","metadata":{"id":"_3NEJ2AywWZh","colab_type":"code","colab":{}},"source":["# model loss\n","\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.title('Model loss : ' + loss)\n","plt.ylabel('Loss')\n","plt.xlabel('Epoch')\n","plt.legend(['Training', 'Validation'], loc='best')\n","plt.show()\n","plt.close()\n","\n","# model accuracy metric\n","\n","plt.plot(np.array(history.history[metrics[0]]))\n","plt.plot(np.array(history.history['val_' + metrics[0]]))\n","plt.title('Model accuracy metric : ' + metrics[0])\n","plt.ylabel('Accuracy metric')\n","plt.xlabel('Epoch')\n","plt.legend(['Training', 'Validation'], loc='best')\n","plt.show()\n","plt.close()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q2efteFFRNJa","colab_type":"text"},"source":["- As iteration goes, the model loss/accuracy move further apart from the training loss/accuracy, which means over fitting happens. Thus, I will use dropout method in the next task."]},{"cell_type":"markdown","metadata":{"id":"LriUvicKFoql","colab_type":"text"},"source":["## Task 2"]},{"cell_type":"code","metadata":{"id":"pPmpTK5OFn6Z","colab_type":"code","colab":{}},"source":["# function to training model\n","def classification_network(\n","    input_train = input_train, \n","    output_train_class_onehot = output_train_class_onehot,\n","    optimizer_type = SGD(lr=0.2),\n","    loss = 'categorical_crossentropy',\n","    metrics = ['categorical_accuracy'],\n","    dropout_ratio = 0.0,\n","    validtrain_split_ratio = 0.2,\n","    max_epochs = 50,\n","    batch_size = 500,\n","    batch_shuffle = True,\n","    num_hidden_nodes = 256,\n","    input_shape = (28, 28, 1),\n","    inputs = Input(shape=input_shape)):\n","\n","    down_01 = Conv2D(filters=16, kernel_size=(3, 3), strides=(1, 1), padding='same')(inputs)\n","    down_01 = Activation('relu')(down_01)\n","    down_01 = Conv2D(filters=16, kernel_size=(3, 3), strides=(1, 1), padding='same')(down_01)\n","    down_01 = Activation('relu')(down_01)\n","\n","    down_01_pool = MaxPooling2D((2, 2), strides=(2, 2))(down_01)   # maxpool downsampled to 14x14x16\n","    down_01_pool = Dropout(dropout_ratio)(down_01_pool)\n","\n","    down_02 = Conv2D(filters=32, kernel_size=(3, 3), strides=(1, 1), padding='same')(down_01_pool)\n","    down_02 = Activation('relu')(down_02)\n","    down_02 = Conv2D(filters=32, kernel_size=(3, 3), strides=(1, 1), padding='same')(down_02)\n","    down_02 = Activation('relu')(down_02)\n","    \n","    down_02_pool = MaxPooling2D((2, 2), strides=(2, 2))(down_02)   # maxpool downsampled to 7x7x32\n","    down_02_pool = Dropout(dropout_ratio)(down_02_pool)\n","\n","    flatten = Flatten()(down_02_pool)   # 1568 nodes\n","\n","    dense_01 = Dense(num_hidden_nodes)(flatten)\n","    dense_01 = Activation('sigmoid')(dense_01)\n","    dense_01 = Dropout(dropout_ratio)(dense_01)\n","\n","    dense_02 = Dense(10)(dense_01)\n","    outputs = Activation('softmax')(dense_02)\n","\n","    # compile the network\n","    model = Model(inputs=inputs, outputs=outputs)\n","    model.compile(optimizer=optimizer_type, loss=loss, metrics=metrics)\n","\n","    print('* Training the compiled network *')\n","    print()\n","\n","    history = model.fit(input_train, output_train_class_onehot, \\\n","                        batch_size=batch_size, \\\n","                        epochs=max_epochs, \\\n","                        validation_split=validtrain_split_ratio, \\\n","                        shuffle=batch_shuffle)\n","    print()\n","    print('Training completed')\n","    print()\n","\n","    return model, history"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vSUMyOFWT2xz","colab_type":"code","colab":{}},"source":["str(R[0])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kO5NKa-hHW0G","colab_type":"code","colab":{}},"source":["# function to plot history\n","def plot_result(history, r):\n","    fig, (axL, axR) = plt.subplots(ncols=2, figsize=(10,4))\n","    # model loss\n","    axL.plot(history.history['loss'])\n","    axL.plot(history.history['val_loss'])\n","    axL.set_title('Loss: Dropout ratio ='+str(r))\n","    axL.set_xlabel('Epoch')\n","    axL.set_ylabel('Loss')\n","    axL.legend(['Training', 'Validation'], loc='best')\n","    # model accuracy metric\n","    axR.plot(np.array(history.history[metrics[0]]))\n","    axR.plot(np.array(history.history['val_' + metrics[0]]))\n","    axR.set_title('Model accuracy: Dropout ratio ='+str(r))\n","    axR.set_ylabel('Accuracy metric')\n","    axR.set_xlabel('Epoch')\n","    axR.legend(['Training', 'Validation'], loc='best')\n","    fig.show()    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yaKJQaNyI4s5","colab_type":"code","colab":{}},"source":["# train a network with different dropout ratio\n","R = [0.1, 0.2, 0.3, 0.4]\n","histories = []\n","for r in R:\n","    model, history = classification_network(dropout_ratio=r)\n","    histories.append(history)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PqrFDMTYLIww","colab_type":"code","colab":{}},"source":["# check the histories\n","for i in range(len(R)):\n","    plot_result(histories[i], R[i])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4oGnLUHYSPro","colab_type":"text"},"source":["Dropout ratio 0.1: Overfitting\n","Dropout ratio 0.3,0.4: Underfitting\n","--> Dropout ratio 0.2 is the optimal value to avoid bot over/underfitting and obtain high accuracy."]},{"cell_type":"code","metadata":{"id":"aoAPVKbqVTLp","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gEX3wuqRYnR_","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}